{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teoría de la información\n",
    "\n",
    "La teoría de la información fue desarrollada originalmente dentro del contexto del cifrado de comunicaciones.\n",
    "\n",
    "Desde el punto de vista de la probabilidad, la noción de información está relacionada a la ocurrencia de los eventos de un fenómeno. En este contexto, un evento da información si:\n",
    "\n",
    "* Es desconocido para nosotros.\n",
    "\n",
    "* A pesar de ser poco probables, terminan ocurriendo.\n",
    "\n",
    "Por ejemplo, si para un día dado, el valor de un índice accionario varía dentro del intervalo $(-1.0\\%, 1.0\\%)$, (algo observado de manera común) entonces la información de este evento es poco relevante. En cambio, si la variación se sitúa dentro del intervalo $(-5\\%, -2\\%)$ (un suceso raramente observado), es claro que algo está ocurriendo en el mercado y por lo tanto, la ocurrencia de este evento, nos brinda mayor información.\n",
    "\n",
    "## Caso discreto\n",
    "\n",
    "**Definición (Información de un punto)**\n",
    "\n",
    "Si $X$ es una variable aleatoria con función de densidad $p$, la información asociada a $x \\in \\mathcal{X}$, se define como:\n",
    "$$\n",
    "I(x) = -\\log  p(x) \n",
    "$$\n",
    "\n",
    "Dentro del contexto de teoría de la información, generalmente, el logaritmo considerado es $log_{2}$, sin embargo, desde el punto de vista de *machine learning*, la base considerada es el logaritmo natural.\n",
    "\n",
    "---------\n",
    "\n",
    "Como consecuencia de la definición de información, si $p(x, y)$ es la función de densidad conjunta de dos variables aleatorias independientes, $X$ y $Y$, entonces la información contenida en dos eventos independientes, es la suma de la información de cada evento\n",
    "$$\n",
    "- log p(x,y) = -log p(x) p(y) = I(x) + I(y)\n",
    "$$\n",
    "\n",
    "------------\n",
    "\n",
    "\n",
    "Dadas dos variables $X, Y$, la cantidad de información provista por la ocurrencia del evento $Y=y$, acerca del evento $X=x$, se mide a través de la **información mútua** la cual se define como\n",
    "\n",
    "**Definición(Información mútua)**\n",
    "\n",
    "$$\n",
    "I(x,y) = log \\dfrac{p(x|y)}{p(x)} = I(y,x).\n",
    "$$\n",
    "\n",
    "De acuerdo a la definición de información mútua, si $X$ y $Y$ son independientes, entonces $I(x,y) = 0$, es decir, la ocurrencia de $y$ no brinda información sobre la ocurrencia de $x$. Por otro lado, si $p(x|y)=1$, entonces $I(x,y) = I(x)$.\n",
    "\n",
    "--------\n",
    "\n",
    "**Definición(Información condicional)**\n",
    "$$\n",
    "I(x|y) = -log p(x|y).\n",
    "$$\n",
    "\n",
    "de acuerdo a esta definición tenemos que \n",
    "$$\n",
    "I(x,y) = I(x) - I(x|y).\n",
    "$$\n",
    "\n",
    "Por lo tanto, la información mútua puede interpretarse como la reducción en la incertidumbre de $x$ por haber observado el valor $y$.\n",
    "\n",
    "------\n",
    "\n",
    "**Definición(Entropía, caso discreto)**\n",
    "\n",
    "La entropía de una variable $X$ se define como\n",
    "$$\n",
    "H(X) = E\\left[- \\log p(x) \\right] = - \\sum_{x \\in \\mathcal{X}}p(x)\\log p(x)\n",
    "$$\n",
    "En el caso de que $p(x)=0$, se define $p(x)\\log p(x) = 0$.\n",
    "\n",
    "La entropía puede interpretarse como la cantidad de información promedio que transmite una variable.\n",
    "\n",
    "En el contexto de teoría de la información, la entropía se considera como una medida de aleatoriedad de una fuente que emite símbolos al azar.\n",
    "\n",
    "**Teorema (Entropía máxima, caso discreto)**\n",
    "\n",
    "En el caso discreto, la distribución uniforme logra la máxima entropía. En esto caso, si $M$ es el número de posibles valores\n",
    "$$\n",
    "H[X] = \\ln M.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio**\n",
    "\n",
    "Programe una función que calcule la entropía de una variable discreta con soporte finito.\n",
    "\n",
    "**NOTA**: No puede utilizar *loops*\n",
    "\n",
    "**Ejercicio**\n",
    "\n",
    "Utilice el ejercicio anterior para comparar la entropía de una variable $Bin(N=5, p = \\tfrac{1}{6})$ y la entropía de una variable uniforme en $\\{0,1,2,3,4,5\\}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import binom\n",
    "\n",
    "def entropia_discreta(p_x):\n",
    "    '''\n",
    "    Función para calcular la entropía de una variable\n",
    "    discreta con soporte finito\n",
    "    \n",
    "    ENTRADA\n",
    "    p_x: np.array con las probabilidades de cada valor\n",
    "    \n",
    "    SALIDA\n",
    "    float que representa la entropía\n",
    "    '''\n",
    "    \n",
    "    return - np.sum(p_x * np.log(p_x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binomial = 1.1566631606305866\n",
      "Uniforme = 1.7917594692280547\n",
      "Ln de n + 1 =  1.791759469228055\n"
     ]
    }
   ],
   "source": [
    "#Una binomial (5, 1/6) toma valores en [0, 1, ... ,5]\n",
    "np.random.seed(54321)\n",
    "n = 5\n",
    "p = 1.0 / 6\n",
    "x = np.array(range(n + 1))\n",
    "p_x_binom = binom.pmf(x, n = 5, p = 1.0/6)\n",
    "entropia_binom = entropia_discreta(p_x_binom)\n",
    "\n",
    "p_x_unif = np.repeat(1.0 /(n + 1) , repeats = n + 1)\n",
    "entropia_unif= entropia_discreta(p_x_unif)\n",
    "\n",
    "print(\"Binomial =\", entropia_binom)\n",
    "print(\"Uniforme =\", entropia_unif)\n",
    "print(\"Ln de n + 1 = \", np.log(n + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analice el siguiente ejemplo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact, fixed\n",
    "import ipywidgets as widgets\n",
    "from scipy.stats import binom\n",
    "\n",
    "def compara_entropia_binom(n1, n2, p = 0.5):\n",
    "    entropia1 = binom.entropy(n = n1, p = p)\n",
    "    entropia2 = binom.entropy(n = n2, p = p)\n",
    "    \n",
    "    #Ojo con np.round\n",
    "    print(\"Entropia Binom 1 =\", np.round(entropia1,4))\n",
    "    print(\"Entropia Binom 2 =\", np.round(entropia2,4))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43af4bc353744a3a8803d6482856fc7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, continuous_update=False, description='n1', max=20), IntSlider(value=0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.compara_entropia_binom(n1, n2, p=0.5)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n1_slider = widgets.IntSlider(min = 0, max = 20, step = 1, continuous_update = False)\n",
    "n2_slider = widgets.IntSlider(min = 0, max = 20, step = 1, continuous_update = False)\n",
    "interact(compara_entropia_binom, n1 = n1_slider, n2 = n2_slider, p = fixed(0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caso continuo\n",
    "\n",
    "**Definición(Entropía, caso continuo)**\n",
    "\n",
    "Si $X$ es una variable aleatoria continua, la entropía (diferencial) de $X$ se define como:\n",
    "\n",
    "$$\n",
    "H[X] = - \\int p(x) \\ln p(x)dx.\n",
    "$$\n",
    "\n",
    "**Teorema(Entropía máxima, caso continuo)**\n",
    "\n",
    "En el caso continuo, la distribución gaussiana maximiza el valor de la entropía. En este caso, si $X \\sim N(\\mu, \\sigma^2)$, entonces\n",
    "\n",
    "$$\n",
    "H[X] = \\dfrac{1}{2}[1 + \\ln(2 \\pi \\sigma^2)]\n",
    "$$\n",
    "\n",
    "**Observación**\n",
    "\n",
    "Como se puede observar a través de la expresión de la entropía de una variable gaussiana, en el caso continuo, la entropía puede tomar valores negativos.\n",
    "\n",
    "**Definición(Entropía condicional, caso continuo)**\n",
    "\n",
    "Suponga que se tiene una distribución conjunta $p(x,y)$. Si se sabe el valor de $Y$, entonces la información adicional necesitada para especificar el valor de $X$ está dad por $-\\ln p(y|x)$. La entropía condicional de $x$ dado $y$ se define como:\n",
    "\n",
    "$$\n",
    "H[X|Y] = - \\int \\int p(x,y) \\ln p(x|y)dxdy.\n",
    "$$\n",
    "\n",
    "Si $H[X,Y]$ denota la entropía diferencial de $p(x,y)$, entonces\n",
    "\n",
    "$$\n",
    "H[X,Y] = H[X|Y] + H[Y]\n",
    "$$\n",
    "\n",
    "**Ejercicio**\n",
    "\n",
    "Utilizando simulación, aproxime la entropía de una variable normal estándar y grafique el resultado de la aproximación utilizando $N$ simulaciones, $N \\in \\{10, 20, 30, ..., 1,000\\}$.\n",
    "\n",
    "Compare sus aproximaciones graficando el valor verdadero utilizando una recta horizontal de color rojo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "np.random.seed(54321)\n",
    "\n",
    "#Eje x está formado por los valores de N\n",
    "N = list(range(10, 1000 + 10, 10))\n",
    "\n",
    "#Para almacenar las aproximaciones\n",
    "ent_aprox = []\n",
    "\n",
    "for i in N:\n",
    "    \n",
    "    #Simula i observaciones\n",
    "    puntos = norm.rvs(size = i)\n",
    "    \n",
    "    #Aproxima entropía\n",
    "    p_x = norm.pdf(puntos)\n",
    "    ent_aprox.append(-np.mean(np.log(p_x)))\n",
    "\n",
    "\n",
    "#Grafica\n",
    "plt.plot(N, ent_aprox, '-')\n",
    "#Línea horizontal\n",
    "plt.hlines(y = norm.entropy(), xmin = N[0], xmax = N[-1],\n",
    "          color = 'red')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Divergencia Kullback-Leibler\n",
    "\n",
    "Imagine que desea estimar una distribución $p(x)$ (desconocida) utilizando como aproximación otra distribución $q(x)$. Para determinar que tan bien es $q$ relativa a $p$ es posible utilizar el cociente\n",
    "\n",
    "$$\n",
    "LR = \\dfrac{p(x)}{q(x)}.\n",
    "$$\n",
    "\n",
    "este cociente nos dice que tan más probable es que el punto $x$ ocurra bajo la distribución $p$ relativo a la distribución $q$. Si $LR>1$, entonces el punto $x$  tiene mayor probabilidad bajo $p$ que bajo $q$, mientras que si $LR < 1$ tenemos el caso contrario.\n",
    "\n",
    "Para una muestra de tamaño $n$, que proviene de $p$, se tiene que \n",
    "\n",
    "$$\n",
    "LR = \\Pi_{i=1}^{n} \\dfrac{p(x_i)} {q(x_i)}.\n",
    "$$\n",
    "\n",
    "\n",
    "o equivalente a\n",
    "\n",
    "$$\n",
    "\\ln LR = \\sum_{i=1}^{n} \\ln \\left[ \\dfrac{p(x_i)}{q(x_i)} \\right]\n",
    "$$\n",
    "\n",
    "por lo tanto, si $q$ es una buena aproximación de $p$, se tiene que $\\ln LR \\approx 0$.\n",
    "\n",
    "\n",
    "No sólo se busca que $LR = 0$ para una muestra de puntos de $p$, si no que buscamos que la igualdad se cumpla para todo $x$ en el soporte de $p$, promediando la ecuación anterior y haciendo que $n \\rightarrow \\infty$, se tiene la siguiente definición.\n",
    "\n",
    "\n",
    "**Definición(Divergencia Kullback-Leibler)**\n",
    "\n",
    "$$\n",
    "KL(p||q) = E_{x \\sim p}\\left[\\ln \\left( \\dfrac{p(x)}{q(x)} \\right) \\right]\n",
    "$$\n",
    "\n",
    "$$\n",
    "KL(p||q) = -\\int p(x) \\ln \\left( \\dfrac{q(x)}{p(x)} \\right)dx\n",
    "$$\n",
    "\n",
    "$$\n",
    "KL(p||q) = \\int p(x) \\ln \\left( \\dfrac{p(x)}{q(x)} \\right)dx\n",
    "$$\n",
    "\n",
    "\n",
    "En otras palabras, la divergencia de Kullback-Leibler nos dice que tan similar es $q$ cuando se evalua sobre muestras que provienen de $p$.\n",
    "\n",
    "**NOTA:**\n",
    "\n",
    "Observe que $KL(p||q) \\neq KL(q||p)$ y por lo tanto, $KL$ no puede considerarse como una métrica.\n",
    "\n",
    "**Teorema(Desigualdad de Jensen)**\n",
    "\n",
    "Si $f$ es una función convexa, entonces se tiene que \n",
    "\n",
    "$$\n",
    "f\\left( E[X] \\right) \\leq E \\left[f(X) \\right].\n",
    "$$\n",
    "\n",
    "**Ejercicio**\n",
    "\n",
    "Utilizando la desigualdad de Jensen, demuestre que $KL(p||q) \\geq 0$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio**\n",
    "\n",
    "Utilizando simulación, calcule la divergencia KL para\n",
    "\n",
    "* $p = N(\\mu = 1, \\sigma = 2)$\n",
    "\n",
    "* $q = N(\\mu = 2, \\sigma = 2)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(54321)\n",
    "\n",
    "#Partición del eje x\n",
    "N = range(10, 1010, 10)\n",
    "\n",
    "#para almacenar los valores estimados para cada tamaño de muestra\n",
    "estimados = []\n",
    "\n",
    "mu_1 = 1\n",
    "mu_2 = 2\n",
    "sig = 2\n",
    "valor_verdadero = ((mu_1 - mu_2)**2) / (2*sig**2)\n",
    "\n",
    "for n in N:\n",
    "    \n",
    "    #Muestra que proviene de la distribución p\n",
    "    x = norm.rvs(loc = 1, scale = 2, size = n)\n",
    "    \n",
    "    #Calcula p(x), q(x)\n",
    "    p = norm.pdf(x = x, loc = mu_1, scale = sig)\n",
    "    q = norm.pdf(x = x, loc = mu_2, scale = sig)\n",
    "    \n",
    "    estimados.append(np.mean(np.log(p/q)))\n",
    "\n",
    "#Crea la gráfica\n",
    "plt.plot(N, estimados, '-', color = 'blue')\n",
    "plt.hlines(y = valor_verdadero, xmin = N[0], xmax = N[-1], color = \"red\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**KL en la práctica**\n",
    "\n",
    "En la práctica, intentamos aproximar a la distribución $p$, utilizando una distribución $q$ parametrizada por un conjunto de parámetros $\\theta$. Supoga que se tienen un conjunto de observaciones $x_1, \\ldots, x_n$ que provienen de $p$, entonces\n",
    "\n",
    "$$\n",
    "KL(p||q) \\approx \\sum_{i=1}^{n} \\left[ -\\ln q(x_i|\\theta) + \\ln p(x_n) \\right]\n",
    "$$\n",
    "\n",
    "ya que el término $\\ln p(x_n)$ no depende de $\\theta$, minimizar la divergencia de Kullback-Liebler es equivalente a maximizar la función de verosimilitud $\\ln q(x_i|\\theta)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
